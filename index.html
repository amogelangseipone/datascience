<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Science Cheat Sheet</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Data Science Cheat Sheet</h1>
    </header>

    <section id="data-inspection">
        <hr/>
        <p>import numpy as np 
import matplotlib.pyplot as plt
            import pandas as pd
            import seaborn as sns
            from sklearn.utils import resample<br/>
            meds = pd.read_csv("medicine.csv")
        meds.head()<br/>

            when dealing with the second parts:<br/>
            -statistical summary - meds.describe(include="all")<br/>
-to change column name: currency.rename(columns = {'Country' : 'CountryOfOrigin'}, inplace = True)<br/>
currency.head()<br/>
-to Display the number of unique values for each feature: currency.nunique()<br/>
            -Getting a concise summary: meds.info()<br/>
            -Checking for any missing values.: meds.isnull().sum()<br/>
-Discard first and last 7 columns: meds = meds.iloc[:, 1:-7]<br/>
            -drop first col: meds = meds.drop(meds.columns[3], axis=1)<br/>
-Count records per medication : meds['Medication'].value_counts() <br/>
            -reducing the sample size of the dataframe w/o replacement: currency = currency.sample(frac = 0.01, replace = False, random_state = 7)<br/>
currency.reset_index(inplace = True, drop = True)<br/>
currency.shape <br/>
</p>
        
    </section>

    <section id="game-outcomes">
       <hr/>
        <p>visualizing the data:<br/>
-box plots: sns.boxplot(data = df[['Weight(mg)', 'Length(mm)', 'Width(mm)', 'Thickness(mm)']])<br/>
plt.show()<br/>
            -to Removes all rows where the 'Thickness(mm)' column has values greater than 1: currency.drop(currency[currency['Thickness(mm)'] > 1].index, inplace = True)<br/>
-scatter plot: <br/>
# Create the scatter plot using seaborn<br/>
sns.scatterplot(data=meds, x='Age', y='Ratio_Na_K', hue='Medication')<br/>
# Add axis labels and title<br/>
plt.xlabel('Age')<br/>
plt.ylabel('Ratio_Na_K')<br/>
plt.title('Scatter Plot of Age vs Ratio_Na_K by Medication')<br/>
plt.show()<br/>
            -categorical plot: sns.countplot(<br/>
    data=dfcus,<br/>
    x="Education_Level",<br/>
    hue="Card_Category"<br/>
)<br/>

# Add necessary labels<br/>
plt.title("Customers by Card Category and Education Level")<br/>
plt.xlabel("Education Level")<br/>
plt.ylabel("Count")<br/>
plt.legend(title="Card Category")<br/>

plt.show()<br/>
            -histogram: sns.histplot(data=meds, x='Age', bins=20, kde=True)<br/>

# Add labels<br/>
plt.xlabel('Age')
plt.ylabel('Number of Patients')<br/>
plt.title('Distribution of Age')<br/>

# Show plot<br/>
plt.show()<br/>

        </p>
        <p>balancing the data: <br/>
            -pie chart: # Count the number of records for each medication<br/>
med_counts = meds['Medication'].value_counts()<br/>

# Create a pie chart<br/>
plt.figure(figsize=(6,6))<br/>
plt.pie(
    med_counts,
    labels=med_counts.index,
    autopct='%.2f%%',  # Round percentages to 2 decimals
    startangle=90
)<br/>
plt.title('Distribution of Medications')<br/>
plt.axis('equal')  # Make the pie chart circular<br/>
plt.show()<br/>
            -underssampling: <br/>
            from sklearn.utils import resample<br/>

# Separate majority (existing) and minority (attrited) classes<br/>
df_majority = dfcus[dfcus['Attrition_Flag'] == 'Existing Customer']<br/>
df_minority = dfcus[dfcus['Attrition_Flag'] == 'Attrited Customer']<br/>

# Undersample the majority class to match minority count<br/>
df_majority_downsampled = resample(df_majority,<br/>
                                   replace=False,    # Without replacement<br/>
                                   n_samples=len(df_minority),  # Match minority size<br/>
                                   random_state=42)  # For reproducibility<br/>

# Combine the downsampled majority with minority<br/>
dfcus_resampled = pd.concat([df_majority_downsampled, df_minority])<br/>

# Verify new class counts<br/>
print(dfcus_resampled['Attrition_Flag'].value_counts())<br/>
            oversampling: from sklearn.utils import resample<br/>

# Step 1: Separate the data by medication class<br/>
classes = [group for _, group in meds.groupby('Medication')]<br/>

# Step 2: Determine the maximum class size<br/>
max_size = max(len(group) for group in classes)<br/>

# Step 3: Oversample each class to match the max size<br/>
resampled_classes = [
    resample(group, replace=True, n_samples=max_size, random_state=1)
    for group in classes
]<br/>

# Step 4: Combine them into one DataFrame<br/>
meds_resampled = pd.concat(resampled_classes)<br/>

# Step 5 (Optional): Shuffle the rows<br/>
meds_resampled = meds_resampled.sample(frac=1, random_state=1).reset_index(drop=True)<br/>

# Check the new class distribution<br/>
print(meds_resampled['Medication'].value_counts())<br/>

        </p>
        
    </section>

    <section id="game-analysis">
       <hr/>
        <p>converting text to numeric vakues:<br/>
binary[0,1] and Convert the columns that contain more than two unique text values to a
vector space using one-hot encoding.:
            <br/>from sklearn.preprocessing import LabelEncoder<br/>

# Binary encode columns with exactly 2 categories<br/>
binary_cols = [col for col in dfcus_resampled.select_dtypes(include=['object']) <br/>
               if dfcus_resampled[col].nunique() == 2]<br/>

for col in binary_cols:<br/>
    le = LabelEncoder()<br/>
    dfcus_resampled[col] = le.fit_transform(dfcus_resampled[col])  # Converts to 0/1 <br/>
        # One-hot encode remaining categorical columns (>2 categories)<br/>
cat_cols = dfcus_resampled.select_dtypes(include=['object']).columns
dfcus_resampled = pd.get_dummies(dfcus_resampled, columns=cat_cols, drop_first=True)<br/>

# Verify transformation<br/>
print("Binary encoded columns:", binary_cols)<br/>
print("\nSample of transformed data:")<br/>
print(dfcus_resampled.iloc[:3, :5])  # Show first 3 rows and 5 columns <br/>

-another way with unique being 1-n, without getdummies:<br/>
            # Loop through all object (text) columns<br/>
for col in meds_resampled.select_dtypes(include='object').columns:<br/>
    
    # If the column has exactly 2 unique values, map them to 0 and 1<br/>
    if meds_resampled[col].nunique() == 2:<br/>
        unique_vals = meds_resampled[col].unique()<br/>
        meds_resampled[col] = meds_resampled[col].map({unique_vals[0]: 0, unique_vals[1]: 1})<br/>
    
    # If the column has more than 2 unique values, convert to numeric codes (0, 1, 2, ...)<br/>
    elif meds_resampled[col].nunique() > 2:<br/>
        meds_resampled[col] = meds_resampled[col].astype('category').cat.codes<br/>

# Optional: Check that all text columns have been converted<br/>
print(meds_resampled.dtypes)<br/>
print(meds_resampled.head())<br/>
        
        Confirm that there are no more columns of type object.:<br/> print(f"Number of columns with data type 'object': {len(currency_clean.dtypes[currency_clean.dtypes == 'object'])}")</p>
        <hr/>
        <p> 
o create a heatmap that depicts the correlation between Medication and the other data columns:<br/> # Compute the correlation matrix<br/>
correlation_matrix = meds_resampled.corr()<br/>

# Create a heatmap of the correlation matrix<br/>
plt.figure(figsize=(10, 8))<br/>
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')<br/>

# Add title<br/>
plt.title('Correlation Heatmap', fontsize=16)<br/>

# Display the plot<br/>
plt.show()<br/>
            interpretation of heat map:<br/> Variable	Correlation with Medication	Interpretation<br/>
Age	0.15	Very weak positive trend (likely negligible).<br/>
Gender	-0.32	Weak negative correlation (e.g., one gender may prefer certain medications).<br/>
Blood_Pressure	0.45	Moderate positive correlation (higher BP linked to specific meds).<br/>
Cholesterol	0.08	No meaningful relationship.<br/>
Ratio_Na_K	0.62	Strong positive correlation (certain meds may affect electrolyte balance).<br/>
</p>
        </section>

    <section id="preprocessing">
        
        <p>training and testing data: <br/>
            -with x and y included:<br/> from sklearn.model_selection import train_test_split<br/>

# Define features and target<br/>
X = meds_resampled.drop('Medication', axis=1)<br/>
y = meds_resampled['Medication']<br/>

# Split into train and test sets (80/20)<br/>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)<br/>

# The input data needs to be scaled,<br/>
# but to avoid data leakage the train and test data must be scaled separately<br/>
# Lose 3 marks if the data is not scaled<br/>
# Lose 2 marks if the data is first scaled and then split<br/>
from sklearn.preprocessing import MinMaxScaler<br/>

scaler = MinMaxScaler()<br/>

# Fit on train, transform both<br/>
scaler.fit(X_train)<br/>
X_train = scaler.transform(X_train)<br/>

scaler.fit(X_test)<br/>
X_test = scaler.transform(X_test)<br/>

# Check the shapes<br/>
print(X_train.shape)<br/>
print(X_test.shape)<br/>
            
-with just xtrain and xtest:<br/> from sklearn.model_selection import train_test_split<br/>

X = currency_clean.drop('Counterfeit', axis = 1)<br/>
y = currency_clean['Counterfeit']<br/>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 7)<br/>

#The input data needs to be scaled, <br/>
#but to avoid data leakage the train and test data needs to be scaled separately<br/>
#Lose 3 marks if the data is not scaled<br/>
#Lose 2 marks if the data is first scaled and then split<br/>
from sklearn.preprocessing import MinMaxScaler<br/>

scaler = MinMaxScaler()<br/>

scaler.fit(X_train)<br/>
X_train = scaler.transform(X_train)<br/>
scaler.fit(X_test)<br/>
X_test = scaler.transform(X_test)<br/>

print(X_train.shape)<br/>
print(X_test.shape)<br/> </p>
        <p>
            training classifiers:<br/>
-with just xtrain and xtest:<br/> • Naïve Bayes (default values).<br/>
• Logistic Regression (default values).<br/>
• Support Vector Machine (gamma = auto).<br/>
• Set k = 5 for the k-fold cross-validation.<br/>
• Report the cross-validated training accuracy and F1 scores
for all the classifiers.<br/>
• Draw a learning curve for each of the classifiers.<br/>
from sklearn.naive_bayes import GaussianNB<br/>
#from sklearn.neighbors import KNeighborsClassifier<br/>
from sklearn.linear_model import LogisticRegression<br/>
#from sklearn.ensemble import RandomForestClassifier<br/>
#from sklearn.tree import DecisionTreeClassifier<br/>
from sklearn.svm import SVC<br/>
from sklearn.model_selection import KFold<br/>
from sklearn.model_selection import cross_val_score<br/>
from sklearn.model_selection import learning_curve<br/>

models = []<br/>

#models.append(('KNN', KNeighborsClassifier()))<br/>
models.append(('LR', LogisticRegression()))<br/>
#models.append(('TREE', DecisionTreeClassifier()))<br/>
#models.append(('FOREST', RandomForestClassifier(n_estimators=5, random_state=42)))<br/>
models.append(('NB', GaussianNB()))<br/>
models.append(('SVM', SVC(gamma = 'auto')))<br/>

results = []<br/>
names = []<br/>

for name, model in models:<br/>
    kfold = KFold(n_splits = 5, random_state = 7, shuffle = True)<br/>
    cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = 'accuracy')<br/>
    results.append(cv_results)<br/>
    names.append(name)<br/>
    msg = '%s(acc): %f (%f)' % (name, cv_results.mean(), cv_results.std())<br/>
    print(msg)<br/>
    cv_results = cross_val_score(model, X_train, y_train, cv = kfold, scoring = 'f1')<br/>
    results.append(cv_results)<br/>
    names.append(name)<br/>
    msg = '%s(f1): %f (%f)' % (name, cv_results.mean(), cv_results.std())<br/>
    print(msg)<br/>
    
    train_sizes, train_scores, validation_scores = learning_curve(estimator = model,
                                                              X = X_train,
                                                              y = y_train,
                                                              train_sizes = np.linspace(0.01, 1.0, 20),
                                                              cv = kfold,
                                                              scoring = 'accuracy')<br/>
    mean_training = np.mean(train_scores, axis=1)<br/>
    mean_testing = np.mean(validation_scores, axis=1)<br/>

    plt.plot(train_sizes, mean_training, '--', color = "b",  label = "Training score")<br/>
    plt.plot(train_sizes, mean_testing, color = "g", label = "Cross-validation score")<br/>
    plt.title(f"Learning Curve for the " + name + " Classifier")<br/>
    plt.xlabel("Training Set Size"), plt.ylabel("Accuracy Score"), plt.legend(loc = "best")<br/>
    plt.tight_layout()<br/>
    plt.show() <br/>
</p>
        
        <hr/>
        <p>
           with those 4 and not 2(idk if it matters): <br/>4 Train the following classifiers and determine the best model by using k-fold
cross-validation: (8)<br/>
o K nearest neighbour (default k-value).<br/>
o Decision tree (default values).<br/>
o Random forest (with 5 trees).<br/>

o Set k = 5 for the k-fold cross-validation.<br/>
o Report the cross-validated training accuracy and F1 scores for all the
classifiers.: #Lose 1 mark for every library that is imported and not used by the student (lose max 3 marks)<br/>
from sklearn.neighbors import KNeighborsClassifier<br/>
from sklearn.tree import DecisionTreeClassifier<br/>
from sklearn.ensemble import RandomForestClassifier<br/>
from sklearn.model_selection import KFold<br/>
from sklearn.model_selection import cross_val_score<br/>
from sklearn.model_selection import learning_curve<br/>

import numpy as np<br/>
import matplotlib.pyplot as plt<br/>

models = []<br/>

# Required classifiers for the question<br/>
models.append(('KNN', KNeighborsClassifier()))  # default k<br/>
models.append(('TREE', DecisionTreeClassifier()))  # default values<br/>
models.append(('FOREST', RandomForestClassifier(n_estimators=5, random_state=42)))  # 5 trees<br/>

results = []<br/>
names = []<br/>

for name, model in models:<br/>
    kfold = KFold(n_splits=5, random_state=7, shuffle=True)<br/>
    
    # Cross-validated Accuracy<br/>
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')<br/>
    results.append(cv_results)<br/>
    names.append(name)<br/>
    msg = '%s (Accuracy): %f (%f)' % (name, cv_results.mean(), cv_results.std())<br/>
    print(msg)<br/>
    
    # Cross-validated F1 Score<br/>
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='f1_weighted')<br/>
    results.append(cv_results)<br/>
    names.append(name)<br/>
    msg = '%s (F1): %f (%f)' % (name, cv_results.mean(), cv_results.std())<br/>
    print(msg)<br/>
    
    # Learning curve<br/>
    train_sizes, train_scores, validation_scores = learning_curve(estimator=model,
                                                                   X=X_train,
                                                                   y=y_train,
                                                                   train_sizes=np.linspace(0.01, 1.0, 20),
                                                                   cv=kfold,
                                                                   scoring='accuracy')<br/>
    mean_training = np.mean(train_scores, axis=1)<br/>
    mean_validation = np.mean(validation_scores, axis=1)<br/>

    plt.plot(train_sizes, mean_training, '--', color="b", label="Training score")<br/>
    plt.plot(train_sizes, mean_validation, color="g", label="Cross-validation score")<br/>
    plt.title(f"Learning Curve for {name} Classifier")<br/>
    plt.xlabel("Training Set Size")<br/>
    plt.ylabel("Accuracy Score")<br/>
    plt.legend(loc="best")<br/>
    plt.grid()<br/>
    plt.tight_layout()<br/>
    plt.show()<br/>

 
        </p> 
        <p>Select the model that produced the highest F1 score to do the
following: (3)<br/>
• Make predictions using the test dataset.<br/>
• Provide the test accuracy score, confusion matrix and
classification report of the model.<br/>
if its svm:<br/> from sklearn import metrics<br/>
from sklearn.metrics import classification_report<br/>
from sklearn.metrics import confusion_matrix<br/>

model = SVC(gamma = 'auto')<br/>
model.fit(X_train, y_train)<br/>
predictions = model.predict(X_test)<br/>

print(metrics.accuracy_score(predictions, y_test))<br/>
print(confusion_matrix(y_test, predictions))<br/>
print(classification_report(y_test, predictions)) 
        
      if its something else:<br/> # Train the model<br/>
model = DecisionTreeClassifier()<br/>
model.fit(X_train, y_train)<br/>

# Make predictions<br/>
predictions = model.predict(X_test)<br/>

# Print metrics<br/>
print(metrics.accuracy_score(predictions, y_test))<br/>
print(confusion_matrix(y_test, predictions))<br/>
print(classification_report(y_test, predictions))<br/>
  
        </p>
            

        <hr/>
    <p> discssing model metrics:<br/>
1. Training vs Testing Accuracy<br/>

The model's training accuracy was likely 100%, and the testing accuracy is 0.989 (or 98.9%).
This small drop suggests that the model generalizes very well and is not overfitting. It performs almost equally well on unseen data, which means it has learned the underlying patterns effectively without memorizing the training data.
<br/>
2. Understanding Precision, Recall, and F1-Score<br/>

Precision is the proportion of predicted positives that are actually correct.<br/>
Example: For class 2, precision is 0.95 – meaning 95% of predicted class 2 values were truly class 2.<br/>

Recall is the proportion of actual positives that were correctly predicted.<br/>
For class 4, recall is 0.95 – meaning 95% of the actual class 4 instances were correctly predicted.<br/>

F1-Score is the harmonic mean of precision and recall, providing a single score that balances both.<br/>
F1-scores are close to 1.00 for all classes, which confirms that the model is doing exceptionally well at both identifying and correctly classifying each class.
<br/>
In conclusion, the model demonstrates high performance, with nearly perfect precision, recall, and F1-scores across all classes. There is no major sign of bias or underperformance for any particular class, which makes this a strong and reliable classifier for the dataset used.
 <br/>   </p>
        <p>
            Given the test accuracy is 0.4915, this is fairly low and indicates that the model isn't performing well on the test data. The F1-score for this model is 0.541829, which is still low, suggesting that the model struggles to correctly classify instances
      <br/> Precision:<br/>
Precision refers to the proportion of positive predictions that were actually correct. It answers the question: Of all the instances the model predicted as positive, how many were actually positive?
<br/>
For Class 0 (negative class), the precision is 0.51. This means that of all the instances predicted as negative, 51% were correct. The higher the precision, the fewer false positives.
<br/>
For Class 1 (positive class), the precision is 0.48, which indicates that 48% of instances predicted as positive were actually correct.
<br/>
Recall:<br/>
Recall refers to the proportion of actual positives that were correctly identified. It answers the question: Of all the instances that were actually positive, how many did the model correctly predict as positive?
<br/>
For Class 0, the recall is 0.34. This means that 34% of the actual negative instances were correctly identified as negative by the model. A lower recall value indicates that many of the actual negatives were missed.
<br/>
For Class 1, the recall is 0.65, meaning the model identified 65% of the actual positive instances correctly. A higher recall indicates that fewer true positives were missed.
<br/>
F1-Score:<br/>
F1-Score is the harmonic mean of precision and recall. It balances the trade-off between precision and recall, especially when you want to consider both false positives and false negatives equally. It's a good metric when you have class imbalance or when both false positives and false negatives are costly.
<br/>
For Class 0, the F1-score is 0.41, which means there is a moderate trade-off between precision and recall, but it's not great.
<br/>
For Class 1, the F1-score is 0.55, which indicates a somewhat better balance between precision and recall.
        <br/>
        </p>
       
        
    </section>

    
</body>
</html>
